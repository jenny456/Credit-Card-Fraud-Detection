{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794  ...   -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974  ...   -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643  ...    0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952  ...   -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074  ...   -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('creditcard.csv')\n",
    "df=df.drop(['Time'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 492 fraud data points and 284315 regular data points.\n"
     ]
    }
   ],
   "source": [
    "frauds = df.loc[df['Class'] == 1]\n",
    "non_frauds = df.loc[df['Class'] == 0]\n",
    "print(\"We have\", len(frauds), \"fraud data points and\", len(non_frauds), \"regular data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794  ...   -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974  ...   -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643  ...    0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952  ...   -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074  ...   -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  0.244964      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724 -0.342475      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  1.160686      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  0.140534      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153 -0.073403      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler=preprocessing.StandardScaler()\n",
    "df[['Amount']]=scaler.fit_transform(df[['Amount']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               V1         V2        V3        V4        V5        V6  \\\n",
      "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
      "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
      "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
      "5       -0.425966   0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
      "6        1.229658   0.141004  0.045371  1.202613  0.191881  0.272708   \n",
      "7       -0.644269   1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
      "8       -0.894286   0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
      "9       -0.338262   1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
      "10       1.449044  -1.176339  0.913860 -1.375667 -1.971383 -0.629152   \n",
      "11       0.384978   0.616109 -0.874300 -0.094019  2.924584  3.317027   \n",
      "12       1.249999  -1.221637  0.383930 -1.234899 -1.485419 -0.753230   \n",
      "13       1.069374   0.287722  0.828613  2.712520 -0.178398  0.337544   \n",
      "14      -2.791855  -0.327771  1.641750  1.767473 -0.136588  0.807596   \n",
      "15      -0.752417   0.345485  2.057323 -1.468643 -1.158394 -0.077850   \n",
      "16       1.103215  -0.040296  1.267332  1.289091 -0.735997  0.288069   \n",
      "17      -0.436905   0.918966  0.924591 -0.727219  0.915679 -0.127867   \n",
      "18      -5.401258  -5.450148  1.186305  1.736239  3.049106 -1.763406   \n",
      "19       1.492936  -1.029346  0.454795 -1.438026 -1.555434 -0.720961   \n",
      "20       0.694885  -1.361819  1.029221  0.834159 -1.191209  1.309109   \n",
      "21       0.962496   0.328461 -0.171479  2.109204  1.129566  1.696038   \n",
      "22       1.166616   0.502120 -0.067300  2.261569  0.428804  0.089474   \n",
      "23       0.247491   0.277666  1.185471 -0.092603 -1.314394 -0.150116   \n",
      "24      -1.946525  -0.044901 -0.405570 -1.013057  2.941968  2.955053   \n",
      "25      -2.074295  -0.121482  1.322021  0.410008  0.295198 -0.959537   \n",
      "26       1.173285   0.353498  0.283905  1.133563 -0.172577 -0.916054   \n",
      "27       1.322707  -0.174041  0.434555  0.576038 -0.836758 -0.831083   \n",
      "28      -0.414289   0.905437  1.727453  1.473471  0.007443 -0.200331   \n",
      "29       1.059387  -0.175319  1.266130  1.186110 -0.786002  0.578435   \n",
      "...           ...        ...       ...       ...       ...       ...   \n",
      "284777   2.079137  -0.028723 -1.343392  0.358000 -0.045791 -1.345452   \n",
      "284778  -0.764523   0.588379 -0.907599 -0.418847  0.901528 -0.760802   \n",
      "284779   1.975178  -0.616244 -2.628295 -0.406246  2.327804  3.664740   \n",
      "284780  -1.727503   1.108356  2.219561  1.148583 -0.884199  0.793083   \n",
      "284781  -1.139015  -0.155510  1.894478 -1.138957  1.451777  0.093598   \n",
      "284782  -0.268061   2.540315 -1.400915  4.846661  0.639105  0.186479   \n",
      "284783  -1.796092   1.929178 -2.828417 -1.689844  2.199572  3.123732   \n",
      "284784  -0.669662   0.923769 -1.543167 -1.560729  2.833960  3.240843   \n",
      "284785   0.032887   0.545338 -1.185844 -1.729828  2.932315  3.401529   \n",
      "284786  -2.076175   2.142238 -2.522704 -1.888063  1.982785  3.732950   \n",
      "284787  -1.029719  -1.110670 -0.636179 -0.840816  2.424360 -2.956733   \n",
      "284788   2.007418  -0.280235 -0.208113  0.335261 -0.715798 -0.751373   \n",
      "284789  -0.446951   1.302212 -0.168583  0.981577  0.578957 -0.605641   \n",
      "284790  -0.515513   0.971950 -1.014580 -0.677037  0.912430 -0.316187   \n",
      "284791  -0.863506   0.874701  0.420358 -0.530365  0.356561 -1.046238   \n",
      "284792  -0.724123   1.485216 -1.132218 -0.607190  0.709499 -0.482638   \n",
      "284793   1.971002  -0.699067 -1.697541 -0.617643  1.718797  3.911336   \n",
      "284794  -1.266580  -0.400461  0.956221 -0.723919  1.531993 -1.788600   \n",
      "284795 -12.516732  10.187818 -8.476671 -2.510473 -4.586669 -1.394465   \n",
      "284796   1.884849  -0.143540 -0.999943  1.506772 -0.035300 -0.613638   \n",
      "284797  -0.241923   0.712247  0.399806 -0.463406  0.244531 -1.343668   \n",
      "284798   0.219529   0.881246 -0.635891  0.960928 -0.152971 -1.014307   \n",
      "284799  -1.775135  -0.004235  1.189786  0.331096  1.196063  5.519980   \n",
      "284800   2.039560  -0.175233 -1.196825  0.234580 -0.008713 -0.726571   \n",
      "284801   0.120316   0.931005 -0.546012 -0.745097  1.130314 -0.235973   \n",
      "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
      "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
      "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
      "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
      "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
      "\n",
      "              V7        V8        V9       V10    ...          V20       V21  \\\n",
      "0       0.239599  0.098698  0.363787  0.090794    ...     0.251412 -0.018307   \n",
      "1      -0.078803  0.085102 -0.255425 -0.166974    ...    -0.069083 -0.225775   \n",
      "2       0.791461  0.247676 -1.514654  0.207643    ...     0.524980  0.247998   \n",
      "3       0.237609  0.377436 -1.387024 -0.054952    ...    -0.208038 -0.108300   \n",
      "4       0.592941 -0.270533  0.817739  0.753074    ...     0.408542 -0.009431   \n",
      "5       0.476201  0.260314 -0.568671 -0.371407    ...     0.084968 -0.208254   \n",
      "6      -0.005159  0.081213  0.464960 -0.099254    ...    -0.219633 -0.167716   \n",
      "7       1.120631 -3.807864  0.615375  1.249376    ...    -0.156742  1.943465   \n",
      "8       0.370145  0.851084 -0.392048 -0.410430    ...     0.052736 -0.073425   \n",
      "9       0.651583  0.069539 -0.736727 -0.366846    ...     0.203711 -0.246914   \n",
      "10     -1.423236  0.048456 -1.720408  1.626659    ...    -0.387226 -0.009302   \n",
      "11      0.470455  0.538247 -0.558895  0.309755    ...     0.125992  0.049924   \n",
      "12     -0.689405 -0.227487 -2.094011  1.323729    ...    -0.102756 -0.231809   \n",
      "13     -0.096717  0.115982 -0.221083  0.460230    ...    -0.153197 -0.036876   \n",
      "14     -0.422911 -1.907107  0.755713  1.151087    ...    -1.582122  1.151663   \n",
      "15     -0.608581  0.003603 -0.436167  0.747731    ...     0.263451  0.499625   \n",
      "16     -0.586057  0.189380  0.782333 -0.267975    ...    -0.113910 -0.024612   \n",
      "17      0.707642  0.087962 -0.665271 -0.737980    ...    -0.047021 -0.194796   \n",
      "18     -1.559738  0.160842  1.233090  0.345173    ...    -2.196848 -0.503600   \n",
      "19     -1.080664 -0.053127 -1.978682  1.638076    ...    -0.387910 -0.177650   \n",
      "20     -0.878586  0.445290 -0.446196  0.568521    ...    -0.138334 -0.295583   \n",
      "21      0.107712  0.521502 -1.191311  0.724396    ...    -0.269321  0.143997   \n",
      "22      0.241147  0.138082 -0.989162  0.922175    ...    -0.307169  0.018702   \n",
      "23     -0.946365 -1.617935  1.544071 -0.829881    ...    -0.230983  1.650180   \n",
      "24     -0.063063  0.855546  0.049967  0.573743    ...    -0.216715 -0.579526   \n",
      "25      0.543985 -0.104627  0.475664  0.149451    ...    -0.386694 -0.403639   \n",
      "26      0.369025 -0.327260 -0.246651 -0.046139    ...     0.027878  0.067003   \n",
      "27     -0.264905 -0.220982 -1.071425  0.868559    ...    -0.522951 -0.284376   \n",
      "28      0.740228 -0.029247 -0.593392 -0.346188    ...     0.097308  0.077237   \n",
      "29     -0.767084  0.401046  0.699500 -0.064738    ...    -0.178023  0.013676   \n",
      "...          ...       ...       ...       ...    ...          ...       ...   \n",
      "284777  0.227476 -0.378355  0.665911  0.028351    ...    -0.272447  0.235758   \n",
      "284778  0.758545  0.414698 -0.730854 -1.245088    ...     0.024870  0.003530   \n",
      "284779 -0.533297  0.842937  1.128798 -0.220744    ...    -0.168378  0.086043   \n",
      "284780 -0.527298  0.866429  0.853819 -0.195152    ...     0.331940 -0.094708   \n",
      "284781  0.191353  0.092211 -0.062621 -0.792066    ...     0.341409 -0.191027   \n",
      "284782 -0.045911  0.936448 -2.419986  0.525012    ...     0.111808 -0.263889   \n",
      "284783 -0.270714  1.657495  0.465804  0.832931    ...     0.319366  0.271170   \n",
      "284784  0.181576  1.282746 -0.893890 -1.453432    ...     0.000965  0.183856   \n",
      "284785  0.337434  0.925377 -0.165663 -0.386953    ...     0.022677 -0.266113   \n",
      "284786 -1.217430 -0.536644  0.272867  0.300342    ...    -0.308523  2.016666   \n",
      "284787  0.283610 -0.332656 -0.247488 -0.328271    ...     0.218776  0.353722   \n",
      "284788 -0.458972 -0.140140  0.959971 -0.028284    ...    -0.143294 -0.208260   \n",
      "284789  1.253430 -1.042610 -0.417116  0.076605    ...    -0.203306  0.851800   \n",
      "284790  0.396137  0.532364 -0.224606 -0.753365    ...    -0.177211 -0.280302   \n",
      "284791  0.757051  0.230473 -0.506856 -1.032990    ...    -0.162132 -0.108846   \n",
      "284792  0.548393  0.343003 -0.226323 -0.348134    ...    -0.077202  0.414621   \n",
      "284793 -1.259306  1.056209  1.315006 -0.146827    ...    -0.153581  0.188758   \n",
      "284794  0.314741  0.004704  0.013857 -0.815911    ...    -0.029539 -0.157831   \n",
      "284795 -3.632516  5.498583  4.893089  8.655320    ...     3.490065 -0.944759   \n",
      "284796  0.190241 -0.249058  0.666458  0.120908    ...    -0.153997  0.144008   \n",
      "284797  0.929369 -0.206210  0.106234 -0.284708    ...    -0.139512 -0.228876   \n",
      "284798  0.427126  0.121340 -0.285670 -0.111640    ...     0.006666  0.099936   \n",
      "284799 -1.518185  2.080825  1.159498 -0.594242    ...     0.348176  0.103302   \n",
      "284800  0.017050 -0.118228  0.435402  0.267772    ...    -0.256922 -0.268048   \n",
      "284801  0.812722  0.115093 -0.204064 -0.657422    ...     0.000676 -0.314205   \n",
      "284802 -4.918215  7.305334  1.914428  4.356170    ...     1.475829  0.213454   \n",
      "284803  0.024330  0.294869  0.584800 -0.975926    ...     0.059616  0.214205   \n",
      "284804 -0.296827  0.708417  0.432454 -0.484782    ...     0.001396  0.232045   \n",
      "284805 -0.686180  0.679145  0.392087 -0.399126    ...     0.127434  0.265245   \n",
      "284806  1.577006 -0.414650  0.486180 -0.915427    ...     0.382948  0.261057   \n",
      "\n",
      "             V22       V23       V24       V25       V26       V27       V28  \\\n",
      "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
      "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
      "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
      "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
      "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
      "5      -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
      "6      -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
      "7      -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
      "8      -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
      "9      -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
      "10      0.313894  0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253   \n",
      "11      0.238422  0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337   \n",
      "12     -0.483285  0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422   \n",
      "13      0.074412 -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   \n",
      "14      0.222182  1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   \n",
      "15      1.353650 -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   \n",
      "16      0.196002  0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   \n",
      "17     -0.672638 -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024   \n",
      "18      0.984460  2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   \n",
      "19     -0.175074  0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602   \n",
      "20     -0.571955 -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499   \n",
      "21      0.402492 -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   \n",
      "22     -0.061972 -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418   \n",
      "23      0.200454 -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   \n",
      "24     -0.799229  0.870300  0.983421  0.321201  0.149650  0.707519  0.014600   \n",
      "25     -0.227404  0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   \n",
      "26      0.227812 -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   \n",
      "27     -0.323357 -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   \n",
      "28      0.457331 -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   \n",
      "29      0.213734  0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "284777  0.829758 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283   \n",
      "284778 -0.431876  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   \n",
      "284779  0.543613 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   \n",
      "284780  0.236818 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   \n",
      "284781 -0.631658 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   \n",
      "284782 -0.857904  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   \n",
      "284783  1.145750  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   \n",
      "284784  0.202670 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   \n",
      "284785 -0.716336  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245   \n",
      "284786 -1.588269  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923   \n",
      "284787  0.488487  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347   \n",
      "284788 -0.430347  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367   \n",
      "284789  0.305268 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   \n",
      "284790 -0.849919  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486   \n",
      "284791 -0.480820 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   \n",
      "284792  1.307511 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692   \n",
      "284793  0.694418  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716   \n",
      "284794 -0.883365  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494   \n",
      "284795 -1.565026  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864   \n",
      "284796  0.634646 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   \n",
      "284797 -0.514376  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265   \n",
      "284798  0.337120  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   \n",
      "284799  0.654850 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   \n",
      "284800 -0.717211  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071   \n",
      "284801 -0.808520  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803   \n",
      "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
      "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
      "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
      "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
      "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
      "\n",
      "          Amount  \n",
      "0       0.244964  \n",
      "1      -0.342475  \n",
      "2       1.160686  \n",
      "3       0.140534  \n",
      "4      -0.073403  \n",
      "5      -0.338556  \n",
      "6      -0.333279  \n",
      "7      -0.190107  \n",
      "8       0.019392  \n",
      "9      -0.338516  \n",
      "10     -0.322044  \n",
      "11     -0.313289  \n",
      "12      0.132538  \n",
      "13     -0.243282  \n",
      "14     -0.118142  \n",
      "15     -0.289300  \n",
      "16     -0.301294  \n",
      "17     -0.349671  \n",
      "18     -0.166119  \n",
      "19     -0.333239  \n",
      "20      0.573167  \n",
      "21     -0.216935  \n",
      "22     -0.344114  \n",
      "23     -0.262273  \n",
      "24     -0.349671  \n",
      "25     -0.247560  \n",
      "26     -0.185790  \n",
      "27     -0.289260  \n",
      "28     -0.221293  \n",
      "29     -0.301294  \n",
      "...          ...  \n",
      "284777 -0.349231  \n",
      "284778 -0.033382  \n",
      "284779 -0.253277  \n",
      "284780 -0.233287  \n",
      "284781 -0.301254  \n",
      "284782 -0.301974  \n",
      "284783 -0.307411  \n",
      "284784 -0.193306  \n",
      "284785 -0.346073  \n",
      "284786 -0.317447  \n",
      "284787 -0.313289  \n",
      "284788 -0.337277  \n",
      "284789 -0.111345  \n",
      "284790 -0.314008  \n",
      "284791 -0.271988  \n",
      "284792 -0.337277  \n",
      "284793 -0.333279  \n",
      "284794 -0.349671  \n",
      "284795 -0.313768  \n",
      "284796 -0.113344  \n",
      "284797 -0.331280  \n",
      "284798 -0.257075  \n",
      "284799 -0.033422  \n",
      "284800 -0.342515  \n",
      "284801 -0.342475  \n",
      "284802 -0.350151  \n",
      "284803 -0.254117  \n",
      "284804 -0.081839  \n",
      "284805 -0.313249  \n",
      "284806  0.514355  \n",
      "\n",
      "[284807 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,0:29]\n",
    "Y=df['Class']\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  (190820, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "190820/190820 [==============================] - 53s 276us/step - loss: 0.0081 - acc: 0.9989\n",
      "Epoch 2/5\n",
      "190820/190820 [==============================] - 56s 296us/step - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 3/5\n",
      "190820/190820 [==============================] - 61s 317us/step - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 4/5\n",
      "190820/190820 [==============================] - 60s 316us/step - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/5\n",
      "190820/190820 [==============================] - 57s 299us/step - loss: 0.0029 - acc: 0.9994\n",
      "93987/93987 [==============================] - 4s 38us/step\n",
      "\n",
      "acc: 99.95%\n",
      "The summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 900\n",
      "Trainable params: 900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(29, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "#model.add(Dense(5, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"The summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.0023879673703647336, 0.99946801153351]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", model.evaluate(X_test.as_matrix(), y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted      0  1  __all__\n",
      "Actual                      \n",
      "0          93821  0    93821\n",
      "1            166  0      166\n",
      "__all__    93987  0    93987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\abstract.py:66: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  df = df.loc[idx, idx.copy()].fillna(0)  # if some columns or rows are missing\n"
     ]
    }
   ],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "y_right = np.array(y_test)\n",
    "y_predicted = model.predict(X_test.as_matrix()).T[0].astype(int)\n",
    "confusion_matrix = ConfusionMatrix(y_right, y_predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\stats.py:60: FutureWarning: supplying multiple axes to axis is deprecated and will be removed in a future version.\n",
      "  num = df[df > 1].dropna(axis=[0, 1], thresh=1).applymap(lambda n: choose(n, 2)).sum().sum() - np.float64(nis2 * njs2) / n2\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\stats.py:60: RuntimeWarning: overflow encountered in longlong_scalars\n",
      "  num = df[df > 1].dropna(axis=[0, 1], thresh=1).applymap(lambda n: choose(n, 2)).sum().sum() - np.float64(nis2 * njs2) / n2\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\stats.py:61: RuntimeWarning: overflow encountered in longlong_scalars\n",
      "  den = (np.float64(nis2 + njs2) / 2 - np.float64(nis2 * njs2) / n2)\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:259: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TN) / self.NegativeTest)\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:304: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.TN + self.FP) * (self.TN + self.FN)))\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:339: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FNR) / self.TNR)\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:251: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FN) / self.NegativeTest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted      0  1  __all__\n",
      "Actual                      \n",
      "0          93821  0    93821\n",
      "1            166  0      166\n",
      "__all__    93987  0    93987\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.998233798291253\n",
      "95% CI: (0.9979440376639886, 0.9984920758004813)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 1.0\n",
      "Kappa: 0.0\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                        0          1\n",
      "Population                                 93987      93987\n",
      "P: Condition positive                      93821        166\n",
      "N: Condition negative                        166      93821\n",
      "Test outcome positive                      93987          0\n",
      "Test outcome negative                          0      93987\n",
      "TP: True Positive                          93821          0\n",
      "TN: True Negative                              0      93821\n",
      "FP: False Positive                           166          0\n",
      "FN: False Negative                             0        166\n",
      "TPR: (Sensitivity, hit rate, recall)           1          0\n",
      "TNR=SPC: (Specificity)                         0          1\n",
      "PPV: Pos Pred Value (Precision)         0.998234        NaN\n",
      "NPV: Neg Pred Value                          NaN   0.998234\n",
      "FPR: False-out                                 1          0\n",
      "FDR: False Discovery Rate              0.0017662        NaN\n",
      "FNR: Miss Rate                                 0          1\n",
      "ACC: Accuracy                           0.998234   0.998234\n",
      "F1 score                                0.999116          0\n",
      "MCC: Matthews correlation coefficient        NaN        NaN\n",
      "Informedness                                   0          0\n",
      "Markedness                                   NaN        NaN\n",
      "Prevalence                              0.998234  0.0017662\n",
      "LR+: Positive likelihood ratio                 1        NaN\n",
      "LR-: Negative likelihood ratio               NaN          1\n",
      "DOR: Diagnostic odds ratio                   NaN        NaN\n",
      "FOR: False omission rate                     NaN  0.0017662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:236: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TP) / self.PositiveTest)\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:267: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FP) / self.PositiveTest)\n",
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:332: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TPR) / self.FPR)\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network After Oversampling , Scaling and PCA (10 components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jenisha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.07316298,  0.07746481,  0.68799046, ...,  0.34358508,\n",
       "        -1.95463673, -0.18842929],\n",
       "       [-2.3227304 , -0.82624077, -0.67298756, ...,  0.16469202,\n",
       "         0.56003508, -1.56186552],\n",
       "       [-2.31833936, -0.1465244 , -0.23057515, ...,  0.01025547,\n",
       "        -0.08772867, -0.32028629],\n",
       "       ...,\n",
       "       [-1.38624254,  0.13835131,  0.27631426, ...,  1.0128357 ,\n",
       "        -0.28064629,  1.42074153],\n",
       "       [ 8.97521933, -2.22494192,  0.03414533, ...,  0.15499723,\n",
       "        -1.53917481,  1.20195829],\n",
       "       [ 1.44315327,  2.98792696, -1.64838549, ...,  0.66236815,\n",
       "        -0.33904962,  1.51535723]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "import imblearn\n",
    "import pandas_ml as pdml\n",
    "df2 = pdml.ModelFrame(X_train, target=y_train)\n",
    "sampler = df2.imbalance.over_sampling.SMOTE()\n",
    "oversampled = df2.fit_sample(sampler)\n",
    "X2, y2 = oversampled.iloc[:,:-1], oversampled['Class']\n",
    "\n",
    "data = scale(X2)\n",
    "pca = PCA(n_components=10)\n",
    "X2 = pca.fit_transform(data)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 27)                297       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                560       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 1,298\n",
      "Trainable params: 1,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(10, input_dim=10, activation='relu')) \n",
    "model2.add(Dense(27, activation='relu'))\n",
    "model2.add(Dense(20, activation='relu'))\n",
    "model2.add(Dense(15, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 380988 samples, validate on 93987 samples\n",
      "Epoch 1/5\n",
      "380988/380988 [==============================] - 47s 123us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 5.8364 - val_acc: 0.3211\n",
      "Epoch 2/5\n",
      "380988/380988 [==============================] - 45s 118us/step - loss: 0.0056 - acc: 0.9986 - val_loss: 5.2036 - val_acc: 0.3917\n",
      "Epoch 3/5\n",
      "380988/380988 [==============================] - 38s 99us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 5.2810 - val_acc: 0.4484\n",
      "Epoch 4/5\n",
      "380988/380988 [==============================] - 40s 104us/step - loss: 0.0043 - acc: 0.9989 - val_loss: 5.2688 - val_acc: 0.4748\n",
      "Epoch 5/5\n",
      "380988/380988 [==============================] - 31s 81us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 5.4318 - val_acc: 0.4611\n"
     ]
    }
   ],
   "source": [
    "X2_test = pca.fit_transform(X_test)\n",
    "h = model2.fit(X2, y2, epochs=5, validation_data=(X2_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [5.431841097605887, 0.4610531243682637]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", model2.evaluate(X2_test, y_test, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted  False   True  __all__\n",
      "Actual                          \n",
      "False      43298  50523    93821\n",
      "True         131     35      166\n",
      "__all__    43429  50558    93987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: 93987\n",
      "P: 166\n",
      "N: 93821\n",
      "PositiveTest: 50558\n",
      "NegativeTest: 43429\n",
      "TP: 35\n",
      "TN: 43298\n",
      "FP: 50523\n",
      "FN: 131\n",
      "TPR: 0.21084337349397592\n",
      "TNR: 0.46149582716023063\n",
      "PPV: 0.0006922742197080581\n",
      "NPV: 0.9969835823988579\n",
      "FPR: 0.5385041728397694\n",
      "FDR: 0.9993077257802919\n",
      "FNR: 0.7891566265060241\n",
      "ACC: 0.4610531243682637\n",
      "F1_score: 0.0013800173487895277\n",
      "MCC: -0.027595845306040392\n",
      "informedness: -0.3276607993457934\n",
      "markedness: -0.0023241433814340517\n",
      "prevalence: 0.0017662017087469544\n",
      "LRP: 0.3915352640298144\n",
      "LRN: 1.7099973175532748\n",
      "DOR: 0.2289683498393068\n",
      "FOR: 0.003016417601142094\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y2_predicted = np.round(model2.predict(X2_test)).T[0]\n",
    "y2_correct = np.array(y_test)\n",
    "confusion_matrix2 = ConfusionMatrix(y2_correct, y2_predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix2)\n",
    "confusion_matrix2.plot(normalized=True)\n",
    "plt.show()\n",
    "confusion_matrix2.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
